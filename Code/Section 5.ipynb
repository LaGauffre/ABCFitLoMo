{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABC on real insurance data \n",
    "\n",
    "In this notebook we are considering three motor insurance data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i ./preamble.py\n",
    "%run -i ./infer_loss_distribution.py\n",
    "%run -i ./infer_count_distribution.py\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dill.load_session(\"Real_Data_Application.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"PyMC3 version:\", pm.__version__)\n",
    "\n",
    "tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST = False\n",
    "\n",
    "# Processor information and SMC calibration parameters\n",
    "if not FAST:\n",
    "    numIters = 15\n",
    "    numItersData = 20\n",
    "    popSize = 1000\n",
    "    popSizeModels = 1000\n",
    "    epsMin = 0\n",
    "    timeout = 1000\n",
    "else:\n",
    "    numIters = 3\n",
    "    numItersData = 5\n",
    "    popSize = 500\n",
    "    popSizeModels = 1000\n",
    "    epsMin = 1\n",
    "    timeout = 30\n",
    "\n",
    "numProcs = 40\n",
    "smcArgs = {\"numProcs\": numProcs, \"timeout\": timeout, \"epsMin\": epsMin, \"verbose\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Australian data set \n",
    "\n",
    "Hereafter the code to import and display the first five observations of the data along with some descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "ausautoBI8999 = pd.read_csv(\"Data/ausautoBI8999.csv\")\n",
    "aus = ausautoBI8999[[\"FinDate\", \"FinMth\", \"AggClaim\"]]\n",
    "print(aus.head(5).to_latex(index=False))\n",
    "print(aus[[\"AggClaim\"]].describe().applymap(\"{:.2e}\".format).to_latex())\n",
    "# df1 = aus.head(5)\n",
    "# df2 = aus.describe()\n",
    "# df1_styler = df1.style.set_table_attributes(\"style='display:inline'\").set_caption('First 5 observations')\n",
    "# df2_styler = df2.style.set_table_attributes(\"style='display:inline'\").set_caption('Descriptive stats')\n",
    "# display_html(df1_styler._repr_html_() +\" \"+ df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of the individual claim sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereafter is an histogram of the claim sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, tight_layout=True)\n",
    "num_bins = 40\n",
    "# the histogram of the data\n",
    "n, bins, patches = axs[0].hist(aus.AggClaim, num_bins, alpha=0.5)\n",
    "axs[1].boxplot(aus.AggClaim)\n",
    "axs[0].set_xlabel(\"Claim size\")\n",
    "axs[0].set_ylabel(\"Counts\")\n",
    "axs[1].set_ylabel(\"Claim size\")\n",
    "axs[1].set_xticks([])\n",
    "# plt.show()\n",
    "sns.despine()\n",
    "# save_cropped(\"../Figures/hist-box-aus.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note from the plots that we have a high proportion of small claims and a couple of large claims. This indicates a long right tail which is typical in insurance data.\n",
    "\n",
    "We consider three models for the claim sizes including \n",
    "- gamma\n",
    "- weibull \n",
    "- lognormal. \n",
    "\n",
    "Given the number of data points (22 036), Bayesian inference is impractible so the parameters of the model via standard methods such as MLE and MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ./infer_loss_distribution.py\n",
    "# The models considered are\n",
    "models_sev = [\"gamma\", \"weibull\", \"lognormal\"]\n",
    "# The first guess for the parameters are\n",
    "θ0 = (1, 1)\n",
    "# The individual claim sizes are fed to the function under a list format\n",
    "\n",
    "uData = list(np.array(aus.AggClaim))\n",
    "# uData = list(np.array(aus.AggClaim)[np.random.choice(len(aus.AggClaim), 69)])\n",
    "\n",
    "mle_summary = infer_loss(uData, models_sev, θ0)\n",
    "print(\n",
    "    pd.concat(\n",
    "        [\n",
    "            mle_summary[[\"model\"]],\n",
    "            mle_summary[[\"param1\", \"param2\", \"BIC\"]].applymap(\"{:.2e}\".format),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).to_latex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = Generator(PCG64(1))\n",
    "μ_mle = mle_summary[mle_summary[\"model\"] == \"lognormal\"].param1.array\n",
    "σ_mle = mle_summary[mle_summary[\"model\"] == \"lognormal\"].param2.array\n",
    "α_mle = mle_summary[mle_summary[\"model\"] == \"gamma\"].param1.array\n",
    "β_mle = mle_summary[mle_summary[\"model\"] == \"gamma\"].param2.array\n",
    "k_mle = mle_summary[mle_summary[\"model\"] == \"weibull\"].param1.array\n",
    "δ_mle = mle_summary[mle_summary[\"model\"] == \"weibull\"].param2.array\n",
    "\n",
    "qs = np.arange(0.01, 0.99, 0.01)\n",
    "\n",
    "df_quantile = pd.DataFrame(\n",
    "    {\n",
    "        \"emp_quant\": pd.Series(uData).quantile(qs),\n",
    "        \"lnorm_quant\": pd.Series(\n",
    "            abcre.simulate_claim_sizes(rg, len(uData), \"lognormal\", (μ_mle, σ_mle))\n",
    "        ).quantile(qs),\n",
    "        \"gamma_quant\": pd.Series(\n",
    "            abcre.simulate_claim_sizes(rg, len(uData), \"gamma\", (α_mle, β_mle))\n",
    "        ).quantile(qs),\n",
    "        \"weibull_quant\": pd.Series(\n",
    "            abcre.simulate_claim_sizes(rg, len(uData), \"weibull\", (k_mle, δ_mle))\n",
    "        ).quantile(qs),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest BIC is reached for the lognormal distribution. The following quantiles-quantiles plot tends to confirm this result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamColor = colors[0]\n",
    "weibColor = colors[1]\n",
    "lnormColor = colors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile-quantile plots\n",
    "fig, axs = plt.subplots(1, 1, figsize=(2.5, 2.5))\n",
    "\n",
    "plt.plot(df_quantile.emp_quant, df_quantile.gamma_quant, c=gamColor, lw=3)\n",
    "plt.plot(df_quantile.emp_quant, df_quantile.weibull_quant, c=weibColor, lw=3)\n",
    "plt.plot(df_quantile.emp_quant, df_quantile.lnorm_quant, c=lnormColor, lw=3)\n",
    "\n",
    "x = np.linspace(0, 3 * 10 ** 5, 100)\n",
    "\n",
    "plt.plot(x, x, \":k\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Empirical Quantiles\")\n",
    "plt.ylabel(\"Theoretical Quantiles\")\n",
    "\n",
    "sns.despine()\n",
    "# save_cropped(\"../Figures/qqplots-aus.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lognormal distribution seems to provide the best fit. We can study the stationarity of the loss distribution over the time period not to be sure to mix up apples and oranges as we aggregate the whole data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_sev = [\"gamma\", \"weibull\", \"lognormal\"]\n",
    "\n",
    "# The first guess for the parameters are\n",
    "θ0 = (1, 1)\n",
    "\n",
    "# The individual claim sizes are fed to the function under a list format\n",
    "time_periods = np.sort(aus.FinMth.value_counts().index)\n",
    "mle_time_periods = pd.concat(\n",
    "    [\n",
    "        infer_loss(\n",
    "            list(np.array(aus.AggClaim[aus.FinMth == time_period])), models_sev, θ0\n",
    "        )\n",
    "        for time_period in time_periods\n",
    "    ]\n",
    ")\n",
    "mle_time_periods[\"FinMth\"] = pd.concat(\n",
    "    [pd.Series(np.repeat(time_period, len(models_sev))) for time_period in time_periods]\n",
    ")\n",
    "mle_time_periods[[\"model\", \"param1\", \"param2\"]].groupby(\"model\").agg([np.mean, np.std])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the model probabilities over the time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    time_periods,\n",
    "    mle_time_periods[mle_time_periods.model == \"gamma\"].model_prob,\n",
    "    c=gamColor,\n",
    "    label=\"Gamma\",\n",
    "    #     marker=\"o\",\n",
    "    #     markersize=6,\n",
    ")\n",
    "plt.plot(\n",
    "    time_periods,\n",
    "    mle_time_periods[mle_time_periods.model == \"weibull\"].model_prob,\n",
    "    c=weibColor,\n",
    "    label=\"Weibull\",\n",
    "    #     marker=\"d\",\n",
    "    #     markersize=6,\n",
    ")\n",
    "plt.plot(\n",
    "    time_periods,\n",
    "    mle_time_periods[mle_time_periods.model == \"lognormal\"].model_prob,\n",
    "    c=lnormColor,\n",
    "    label=\"Lognormal\",\n",
    "    #     marker=\"s\",\n",
    "    #     markersize=6,\n",
    ")\n",
    "plt.xlabel(\"Time Period\")\n",
    "# plt.title(\"Model Evidence\")\n",
    "# plt.legend(loc=\"right\", frameon=False)\n",
    "sns.despine()\n",
    "# save_cropped(\"../Figures/model-prob-aus.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lognormal model seems to be consistently the best one over all the time period. To observe if there is some stationarity, we investigate the behavior the parameters accross the time periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, tight_layout=True)\n",
    "\n",
    "# the histogram of the data\n",
    "axs[0].plot(\n",
    "    time_periods, mle_time_periods[mle_time_periods.model == \"gamma\"].param1, c=gamColor\n",
    ")\n",
    "axs[0].set_xlabel(\"Time Period\")\n",
    "# axs[0].set_title(\"$r$\")\n",
    "axs[0].axhline(y=α_mle, **mleStyle)\n",
    "\n",
    "axs[1].plot(\n",
    "    time_periods, mle_time_periods[mle_time_periods.model == \"gamma\"].param2, c=gamColor\n",
    ")\n",
    "axs[1].set_xlabel(\"Time Period\")\n",
    "# axs[1].set_title(\"$m$\")\n",
    "axs[1].axhline(y=β_mle, **mleStyle)\n",
    "\n",
    "sns.despine()\n",
    "# save_cropped(\"../Figures/aus-gamma-params.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both parameter demonstrate large variation accross the time period, potentially due to the lack of fit of the gamma disribution to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, tight_layout=True)\n",
    "\n",
    "# the histogram of the data\n",
    "axs[0].plot(\n",
    "    time_periods,\n",
    "    mle_time_periods[mle_time_periods.model == \"weibull\"].param1,\n",
    "    c=weibColor,\n",
    ")\n",
    "axs[0].set_xlabel(\"Time Period\")\n",
    "# axs[0].set_title(\"$k$\")\n",
    "axs[0].axhline(y=k_mle, **mleStyle)\n",
    "\n",
    "axs[1].plot(\n",
    "    time_periods,\n",
    "    mle_time_periods[mle_time_periods.model == \"weibull\"].param2,\n",
    "    c=weibColor,\n",
    ")\n",
    "axs[1].set_xlabel(\"Time Period\")\n",
    "# axs[1].set_title(\"$\\\\beta$\")\n",
    "axs[1].axhline(y=δ_mle, **mleStyle)\n",
    "\n",
    "sns.despine()\n",
    "# save_cropped(\"../Figures/aus-weibull-params.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for the gamma distribution the parameters of the weibull distribution exhibit a high volatility. These results are consistent with the analysis of the overall data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, tight_layout=True)\n",
    "\n",
    "# the histogram of the data\n",
    "axs[0].plot(\n",
    "    time_periods,\n",
    "    mle_time_periods[mle_time_periods.model == \"lognormal\"].param1,\n",
    "    c=lnormColor,\n",
    ")\n",
    "axs[0].set_xlabel(\"Time Period\")\n",
    "# axs[0].set_title(\"$\\mu$\")\n",
    "axs[0].axhline(y=μ_mle, **mleStyle)\n",
    "axs[1].plot(\n",
    "    time_periods,\n",
    "    mle_time_periods[mle_time_periods.model == \"lognormal\"].param2,\n",
    "    c=lnormColor,\n",
    ")\n",
    "axs[1].set_xlabel(\"Time Period\")\n",
    "# axs[1].set_title(\"$\\sigma$\")\n",
    "axs[1].axhline(y=σ_mle, **mleStyle)\n",
    "\n",
    "sns.despine()\n",
    "# save_cropped(\"../Figures/aus-log-norm-params.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the lognormal distribution remains reasonably stable across the time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we aggregate the data in order to have only information per time period (monthly aggregation). hereafter a short overview of the data and some descriptive stat over the number of claims and the total claim amount per time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = pd.DataFrame(\n",
    "    {\"N\": aus.groupby(by=\"FinMth\").size(), \"X\": aus.groupby(by=\"FinMth\").AggClaim.sum()}\n",
    ")\n",
    "\n",
    "print(\n",
    "    pd.concat(\n",
    "        [df_agg[\"N\"].head(), df_agg[\"X\"].head().apply(\"{:.2e}\".format)], axis=1\n",
    "    ).to_latex()\n",
    ")\n",
    "print(\n",
    "    pd.concat([aus[\"AggClaim\"].describe(), df_agg.describe()], axis=1)\n",
    "    .applymap(\"{:.2e}\".format)\n",
    "    .to_latex()\n",
    ")\n",
    "# df1 = df_agg.head(7)\n",
    "# df2 = df_agg.describe()\n",
    "# df1_styler = df1.style.set_table_attributes(\"style='display:inline'\").set_caption('First 7 observations')\n",
    "# df2_styler = df2.style.set_table_attributes(\"style='display:inline'\").set_caption('Descriptive stats')\n",
    "# display_html(df1_styler._repr_html_() +\" \"+ df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of the claim frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, tight_layout=True)\n",
    "num_bins = 20\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = axs[0].hist(df_agg.N, num_bins, alpha=0.5)\n",
    "axs[1].boxplot(df_agg.N)\n",
    "axs[0].set_xlabel(\"Claim Frequency\")\n",
    "axs[0].set_ylabel(\"Counts\")\n",
    "axs[1].set_ylabel(\"Claim Frequency\")\n",
    "axs[1].set_xticks([])\n",
    "\n",
    "sns.despine()\n",
    "# save_cropped(\"../Figures/hist-box-claim-frequency.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a Bayesian analysis over the claim frequency, considering two models including\n",
    "- Poisson\n",
    "- negative binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "nData = df_agg.N.values\n",
    "\n",
    "with pm.Model() as model_negbin:\n",
    "    α = pm.Uniform(\"α\", lower=0, upper=20)\n",
    "    p = pm.Uniform(\"p\", lower=1e-3, upper=1)\n",
    "    N = pm.NegativeBinomial(\"N\", mu=α * (1 - p) / p, alpha=α, observed=nData)\n",
    "    %time trace = pm.sample_smc(popSize, random_seed=1)\n",
    "\n",
    "post_sample_negbin = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": np.repeat(\"negative binomial\", popSize),\n",
    "        \"weights\": np.ones(popSize) / popSize,\n",
    "        \"α\": trace[\"α\"],\n",
    "        \"p\": trace[\"p\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "with pm.Model() as model_poisson:\n",
    "    λ = pm.Uniform(\"λ\", lower=300, upper=350)\n",
    "    N = pm.Poisson(\"N\", mu=λ, observed=nData)\n",
    "    %time trace = pm.sample_smc(popSize, random_seed=1)\n",
    "\n",
    "post_sample_poisson = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": np.repeat(\"poisson\", popSize),\n",
    "        \"weights\": np.ones(popSize) / popSize,\n",
    "        \"λ\": trace[\"λ\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_likelihood = np.exp(\n",
    "    np.array(\n",
    "        [model_negbin.marginal_log_likelihood, model_poisson.marginal_log_likelihood]\n",
    "    )\n",
    ")\n",
    "BF = marginal_likelihood / np.max(marginal_likelihood)\n",
    "Bayesian_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"models\": [\"negative binomial\", \"poisson\"],\n",
    "        \"param1\": [post_sample_negbin[\"α\"].mean(), post_sample_poisson[\"λ\"].mean()],\n",
    "        \"param2\": [post_sample_negbin[\"p\"].mean(), None],\n",
    "        \"BF\": BF,\n",
    "        \"model_prob\": BF / np.sum(BF),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the MLE of the lognormal distribution and the map of the negative binomial distribution as the target parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ_mle = np.array(\n",
    "    [\n",
    "        [\n",
    "            post_sample_negbin[\"α\"].mean(),\n",
    "            post_sample_negbin[\"p\"].mean(),\n",
    "            μ_mle[0],\n",
    "            σ_mle[0],\n",
    "        ],\n",
    "        [\n",
    "            post_sample_negbin[\"α\"].mean(),\n",
    "            post_sample_negbin[\"p\"].mean(),\n",
    "            α_mle[0],\n",
    "            β_mle[0],\n",
    "        ],\n",
    "        [\n",
    "            post_sample_negbin[\"α\"].mean(),\n",
    "            post_sample_negbin[\"p\"].mean(),\n",
    "            k_mle[0],\n",
    "            δ_mle[0],\n",
    "        ],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABC posterior of a negative binomial -lognormal model\n",
    "\n",
    "### ABC posterior of a negative binomial -lognormal model without the claim frequency\n",
    "\n",
    "\n",
    "We will now use ABC on the aggregate data to fit a compound \n",
    "\n",
    "- negative binomial-lognormal\n",
    "\n",
    "We start by the first data regime where we do not know anything about the claim frequency. We only consider the aggregated data. \n",
    "\n",
    "Let us start by infer the negative binomial lognormal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (\"α\", \"p\", \"μ\", \"σ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = abcre.Psi(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lame_prior = abcre.IndependentUniformPrior(\n",
    "    ((0, 20), (1e-3, 1), (0, 20), (0, 10)), params\n",
    ")\n",
    "great_prior = abcre.IndependentUniformPrior(\n",
    "    ((0, 20), (1e-3, 1), (-10, 10), (0, 10)), params\n",
    ")\n",
    "\n",
    "model_lame = abcre.Model(\"negative binomial\", \"lognormal\", psi, lame_prior)\n",
    "model_great = abcre.Model(\"negative binomial\", \"lognormal\", psi, great_prior)\n",
    "\n",
    "xData = df_agg.X.values\n",
    "\n",
    "%time fit_lame = abcre.smc(numIters, popSize, xData, model_lame, **smcArgs)\n",
    "%time fit_great = abcre.smc(numIters, popSize, xData, model_great, **smcArgs)\n",
    "\n",
    "abc_sample_lame_prior = pd.DataFrame(\n",
    "    {\n",
    "        \"prior\": np.repeat(\"lame\", popSize),\n",
    "        \"model_freq\": np.repeat(\"negative binomial\", popSize),\n",
    "        \"model_sev\": np.repeat(\"lognormal\", popSize),\n",
    "        \"weights\": fit_lame.weights,\n",
    "        \"α\": fit_lame.samples[:, 0],\n",
    "        \"p\": fit_lame.samples[:, 1],\n",
    "        \"μ\": fit_lame.samples[:, 2],\n",
    "        \"σ\": fit_lame.samples[:, 3],\n",
    "    }\n",
    ")\n",
    "\n",
    "abc_sample_great_prior = pd.DataFrame(\n",
    "    {\n",
    "        \"prior\": np.repeat(\"great\", popSize),\n",
    "        \"model_freq\": np.repeat(\"negative binomial\", popSize),\n",
    "        \"model_sev\": np.repeat(\"lognormal\", popSize),\n",
    "        \"weights\": fit_great.weights,\n",
    "        \"α\": fit_great.samples[:, 0],\n",
    "        \"p\": fit_great.samples[:, 1],\n",
    "        \"μ\": fit_great.samples[:, 2],\n",
    "        \"σ\": fit_great.samples[:, 3],\n",
    "    }\n",
    ")\n",
    "abc_sample_nb_lnorm = pd.concat([abc_sample_lame_prior, abc_sample_great_prior], axis=0)\n",
    "\n",
    "abc_sample_nb_lnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of the parammeters of the negative binomial distribution\n",
    "prior_names = [\"lame\", \"great\"]\n",
    "# prior_colors = [\"tab:olive\", \"tab:brown\"]\n",
    "prior_colors = [colors[0], colors[2]]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(params), tight_layout=True)\n",
    "linestyles = [\"solid\", \"solid\"]\n",
    "\n",
    "for l in range(len(params)):\n",
    "    for k in range(len(prior_names)):\n",
    "        sample = abc_sample_nb_lnorm[abc_sample_nb_lnorm.prior == prior_names[k]][\n",
    "            params[l]\n",
    "        ]\n",
    "        weights = abc_sample_nb_lnorm[abc_sample_nb_lnorm.prior == prior_names[k]][\n",
    "            \"weights\"\n",
    "        ]\n",
    "        dataResampled, xs, ys = abcre.resample_and_kde(sample, weights)\n",
    "        axs[l].plot(xs, ys, color=prior_colors[k], linestyle=linestyles[k])\n",
    "        axs[l].axvline(θ_mle[0, l], **mleStyle)\n",
    "        # axs[l].set_title(\"$\" + params[l] + \"$\")\n",
    "        axs[l].set_yticks([])\n",
    "\n",
    "sns.despine(left=True)\n",
    "# save_cropped(\"../Figures/hist-RD-negbin-lognormal-priors.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABC posterior of a negative binomial -lognormal model with the claim frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (\"μ\", \"σ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lame_prior = abcre.IndependentUniformPrior(((0, 20), (0, 10)), (\"μ\", \"σ\"))\n",
    "great_prior = abcre.IndependentUniformPrior(((-10, 10), (0, 10)), (\"μ\", \"σ\"))\n",
    "\n",
    "xData = df_agg.X.values\n",
    "nData = df_agg.N.values\n",
    "\n",
    "model_lame = abcre.Model(nData, \"lognormal\", psi, lame_prior)\n",
    "model_great = abcre.Model(nData, \"lognormal\", psi, great_prior)\n",
    "\n",
    "%time fit_lame = abcre.smc(numItersData, popSize, xData, model_lame, **smcArgs)\n",
    "%time fit_great = abcre.smc(numItersData, popSize, xData, model_great, **smcArgs)\n",
    "\n",
    "abc_sample_lame_prior = pd.DataFrame(\n",
    "    {\n",
    "        \"prior\": np.repeat(\"lame\", popSize),\n",
    "        \"model_freq\": np.repeat(\"With Frequencies\", popSize),\n",
    "        \"model_sev\": np.repeat(\"lognormal\", popSize),\n",
    "        \"weights\": fit_lame.weights,\n",
    "        \"μ\": fit_lame.samples[:, 0],\n",
    "        \"σ\": fit_lame.samples[:, 1],\n",
    "    }\n",
    ")\n",
    "\n",
    "abc_sample_great_prior = pd.DataFrame(\n",
    "    {\n",
    "        \"prior\": np.repeat(\"great\", popSize),\n",
    "        \"model_freq\": np.repeat(\"With Frequencies\", popSize),\n",
    "        \"model_sev\": np.repeat(\"lognormal\", popSize),\n",
    "        \"weights\": fit_great.weights,\n",
    "        \"μ\": fit_great.samples[:, 0],\n",
    "        \"σ\": fit_great.samples[:, 1],\n",
    "    }\n",
    ")\n",
    "\n",
    "abc_sample_freqs_lnorm = pd.concat(\n",
    "    [abc_sample_lame_prior, abc_sample_great_prior], axis=0\n",
    ")\n",
    "abc_sample_freqs_lnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate the posterior sample of the lognormal distribution when we have the claim frequency data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of the parameters of the negative binomial distribution\n",
    "abc_sample_lnorm = pd.concat(\n",
    "    [\n",
    "        abc_sample_nb_lnorm[[\"prior\", \"model_freq\", \"model_sev\", \"weights\", \"μ\", \"σ\"]],\n",
    "        abc_sample_freqs_lnorm,\n",
    "    ]\n",
    ")\n",
    "\n",
    "prior_names = [\"lame\", \"great\"]\n",
    "freq_models = [\"negative binomial\", \"With Frequencies\"]\n",
    "alphas = [0.5, 1]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(params), tight_layout=True)\n",
    "\n",
    "for l in range(len(params)):\n",
    "    for k in range(len(prior_names)):\n",
    "        for i in range(len(freq_models)):\n",
    "            selector = (abc_sample_lnorm.model_freq == freq_models[i]) & (\n",
    "                abc_sample_lnorm.prior == prior_names[k]\n",
    "            )\n",
    "            sample = abc_sample_lnorm[selector][params[l]]\n",
    "            weights = abc_sample_lnorm[selector][\"weights\"]\n",
    "\n",
    "            dataResampled, xs, ys = abcre.resample_and_kde(sample, weights)\n",
    "            axs[l].plot(xs, ys, color=prior_colors[k], alpha=alphas[i])\n",
    "            axs[l].axvline(θ_mle[0, l + 2], **mleStyle)\n",
    "            # axs[l].set_title(\"$\"+ params[l] + \"$\")\n",
    "            axs[l].set_yticks([])\n",
    "\n",
    "sns.despine(left=True)\n",
    "# save_cropped(\"../Figures/hist-RD-freq-lognormal-priors.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABC model probabilities\n",
    "### ABC model probabilities without the claim frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(\"α\", \"p\", \"μ\", \"σ\"), (\"α\", \"p\", \"r\", \"m\"), (\"α\", \"p\", \"k\", \"β\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xData = df_agg.X.values\n",
    "nData = df_agg.N.values\n",
    "\n",
    "prior1 = abcre.IndependentUniformPrior(\n",
    "    [(0, 20), (1e-3, 1), (5, 10), (0, 3)], (\"α\", \"p\", \"μ\", \"σ\")\n",
    ")\n",
    "model1 = abcre.Model(\"negative binomial\", \"lognormal\", psi, prior1)\n",
    "\n",
    "prior2 = abcre.IndependentUniformPrior(\n",
    "    [(0, 20), (1e-3, 1), (0, 100), (0, 150000)], (\"α\", \"p\", \"r\", \"m\")\n",
    ")\n",
    "model2 = abcre.Model(\"negative binomial\", \"gamma\", psi, prior2)\n",
    "\n",
    "prior3 = abcre.IndependentUniformPrior(\n",
    "    [(0, 20), (1e-3, 1), (1e-3, 1), (0, 40000)], (\"α\", \"p\", \"k\", \"β\")\n",
    ")\n",
    "model3 = abcre.Model(\"negative binomial\", \"weibull\", psi, prior3)\n",
    "\n",
    "models = [model1, model2, model3]\n",
    "\n",
    "sev_models = [\"lognormal\", \"gamma\", \"weibull\"]\n",
    "\n",
    "abc_model_prob_nb = pd.DataFrame(\n",
    "    {\"model_freq\": [], \"model_sev\": [], \"model_probability\": []}\n",
    ")\n",
    "abc_model_sample_nb = pd.DataFrame(\n",
    "    {\n",
    "        \"model_freq\": [],\n",
    "        \"model_sev\": [],\n",
    "        \"weights\": [],\n",
    "        \"α\": [],\n",
    "        \"p\": [],\n",
    "        \"param1\": [],\n",
    "        \"param2\": [],\n",
    "    }\n",
    ")\n",
    "\n",
    "%time fit = abcre.smc(numIters, popSizeModels, xData, models, **smcArgs)\n",
    "\n",
    "for k in range(len(sev_models)):\n",
    "    weights = fit.weights[fit.models == k]\n",
    "    res_mp = pd.DataFrame(\n",
    "        {\n",
    "            \"model_freq\": pd.Series([\"negative binomial\"]),\n",
    "            \"model_sev\": pd.Series([sev_models[k]]),\n",
    "            \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    abc_model_prob_nb = pd.concat([abc_model_prob_nb, res_mp])\n",
    "\n",
    "    res_post_samples = pd.DataFrame(\n",
    "        {\n",
    "            \"model_freq\": np.repeat(\"negative binomial\", len(weights)),\n",
    "            \"model_sev\": np.ravel(np.repeat(sev_models[k], len(weights))),\n",
    "            \"weights\": weights / np.sum(weights),\n",
    "            \"α\": np.array(fit.samples)[fit.models == k, 0],\n",
    "            \"p\": np.array(fit.samples)[fit.models == k, 1],\n",
    "            \"param1\": np.array(fit.samples)[fit.models == k, 2],\n",
    "            \"param2\": np.array(fit.samples)[fit.models == k, 3],\n",
    "        }\n",
    "    )\n",
    "    abc_model_sample_nb = pd.concat([abc_model_sample_nb, res_post_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABC model probabilities with the claim frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xData = df_agg.X.values\n",
    "nData = df_agg.N.values\n",
    "\n",
    "prior1 = abcre.IndependentUniformPrior([(5, 10), (0, 3)], (\"μ\", \"σ\"))\n",
    "model1 = abcre.Model(nData, \"lognormal\", psi, prior1)\n",
    "\n",
    "prior2 = abcre.IndependentUniformPrior([(0, 100), (0, 150000)], (\"r\", \"m\"))\n",
    "model2 = abcre.Model(nData, \"gamma\", psi, prior2)\n",
    "\n",
    "prior3 = abcre.IndependentUniformPrior([(0, 1), (0, 40000)], (\"k\", \"β\"))\n",
    "model3 = abcre.Model(nData, \"weibull\", psi, prior3)\n",
    "\n",
    "models = [model1, model2, model3]\n",
    "\n",
    "sev_models = [\"lognormal\", \"gamma\", \"weibull\"]\n",
    "\n",
    "abc_model_prob_freqs = pd.DataFrame(\n",
    "    {\"model_freq\": [], \"model_sev\": [], \"model_probability\": []}\n",
    ")\n",
    "abc_model_sample_freqs = pd.DataFrame(\n",
    "    {\"model_freq\": [], \"model_sev\": [], \"weights\": [], \"param1\": [], \"param2\": []}\n",
    ")\n",
    "\n",
    "\n",
    "%time fit = abcre.smc(numItersData, popSizeModels, xData, models, **smcArgs)\n",
    "\n",
    "for k in range(len(sev_models)):\n",
    "    weights = fit.weights[fit.models == k]\n",
    "    res_mp = pd.DataFrame(\n",
    "        {\n",
    "            \"model_freq\": pd.Series([\"With Frequencies\"]),\n",
    "            \"model_sev\": pd.Series([sev_models[k]]),\n",
    "            \"model_probability\": pd.Series(np.sum(fit.weights[fit.models == k])),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    abc_model_prob_freqs = pd.concat([abc_model_prob_freqs, res_mp])\n",
    "\n",
    "    res_post_samples = pd.DataFrame(\n",
    "        {\n",
    "            \"model_freq\": np.repeat(\"With Frequencies\", len(weights)),\n",
    "            \"model_sev\": np.ravel(np.repeat(sev_models[k], len(weights))),\n",
    "            \"weights\": weights / np.sum(weights),\n",
    "            \"param1\": np.array(fit.samples)[fit.models == k, 0],\n",
    "            \"param2\": np.array(fit.samples)[fit.models == k, 1],\n",
    "        }\n",
    "    )\n",
    "    abc_model_sample_freqs = pd.concat([abc_model_sample_freqs, res_post_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "abc_model_prob = pd.concat([abc_model_prob_freqs, abc_model_prob_nb])\n",
    "\n",
    "abc_model_prob = abc_model_prob.replace(\n",
    "    {\n",
    "        \"lognormal\": \"Lognormal\",\n",
    "        \"gamma\": \"Gamma\",\n",
    "        \"weibull\": \"Weibull\",\n",
    "        \"negative binomial\": \"Negative Binomial\",\n",
    "    }\n",
    ")\n",
    "abc_model_prob.columns = [\"Model Frequency\", \"Model Severity\", \"Model Probability\"]\n",
    "\n",
    "# g = sns.catplot(\n",
    "#     x=\"Model Frequency\",\n",
    "#     y=\"Model Probability\",\n",
    "#     hue=\"Model Severity\",\n",
    "#     data=abc_model_prob,\n",
    "#     kind=\"bar\",\n",
    "# )\n",
    "# plt.ylabel(\"\")\n",
    "# plt.xlabel(\"Frequency model\")\n",
    "# plt.title(\"Model evidence\")\n",
    "\n",
    "# plt.legend(title=\"\")\n",
    "# save_cropped(\"../Figures/barplot-RD-model-selection.pdf\")\n",
    "\n",
    "print(\n",
    "    pd.pivot_table(\n",
    "        abc_model_prob,\n",
    "        values=\"Model Probability\",\n",
    "        index=[\"Model Frequency\"],\n",
    "        columns=[\"Model Severity\"],\n",
    "        aggfunc=np.sum,\n",
    "    ).to_latex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed = toc()\n",
    "print(f\"Notebook time = {elapsed:.0f} secs = {elapsed/60:.2f} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session(\"Real_Data_Application.pkl\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
